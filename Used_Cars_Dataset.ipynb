{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment #1\n",
    "Module code: ID5059\n",
    "\n",
    "Module: Knowledge Discovery and Datamining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required \n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scikit-learn | grep -v 'already satisfied'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data by reading the dataset from a saved locally\n",
    "# Tested on larger datasets, but submitted on the smaller one for data size considerations\n",
    "used_cars_data = pd.read_csv(\"data/1_small/used_cars_data_small_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some basic understanding of the data and its structure\n",
    "print(used_cars_data.head(10))\n",
    "print(used_cars_data.info())\n",
    "print(used_cars_data.describe(include=\"all\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "\n",
    "In obtaining some understanding of what the data looks like, it becomes evident that data cleaning is necessary for useful results to be yielded later in the machine learning process. There are a lot of variables, data types, some of which are not useful in predicting the price, missing values and different formatting. As such, the data is cleaned before it can be used for computations. For this assignment, I have chosen to keep a separate variable for the used cars for the data exploration and data cleaning, so that I can go back and look at the results later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the columns in the dataset had all NaN values. These will in no way be helpful to future computations, and are therefore removed\n",
    "# We are then left with 63 columns instad of 66, when using the used_cars_data_small_0.csv dataset\n",
    "used_cars_data.dropna(axis=1, inplace=True, how=\"all\")\n",
    "\n",
    "# However, we might want to run this code on other datasets\n",
    "# Therefore, we add contingency code to drop the tables anyways, should they still exist\n",
    "columns_to_drop = [\"combine_fuel_economy\", \"is_certified\", \"vehicle_damage_category\"]\n",
    "for col in columns_to_drop:\n",
    "    if col in used_cars_data.columns:\n",
    "        used_cars_data.drop(col, axis=1, inplace=True, how=\"all\")\n",
    "\n",
    "# Attributes that likely have no significance on calculating price are also dropped\n",
    "used_cars_data = used_cars_data.drop(labels=[\"is_cpo\", \"is_oemcpo\", \"sp_id\", \"exterior_color\", \"listing_id\", \"description\", \"main_picture_url\", \"vin\", \"bed_height\", \"bed_length\", \"cabin\"], axis=1)\n",
    "\n",
    "# Some of the values of attributes are indicated as missing by the \"--\" notation\n",
    "used_cars_data.replace(\"--\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A second look at the data frame reveales that some values will be complicated to work with\n",
    "# Even if processed through mapping of categories, some attributes have too many unique values to be worth the efforts, and are not useful in predicting the price anyways\n",
    "# Such attributes are therefore dropped too\n",
    "used_cars_data = used_cars_data.drop(labels=[\"bed\", \"fleet\", \"frame_damaged\",\"franchise_make\", \"isCab\", \"has_accidents\", \"owner_count\", \"salvage\", \"theft_title\", \"highway_fuel_economy\", \"transmission_display\", \"torque\"], axis=1)\n",
    "\n",
    "# Some attributes are not useful in the light of other attributes, such as \"model_name,\" since \"make_name\" is sufficient in this context\n",
    "used_cars_data = used_cars_data.drop(labels=[\"engine_cylinders\", \"engine_displacement\", \"listed_date\", \"model_name\", \"interior_color\", \"major_options\", \"trimId\", \"trim_name\", \"dealer_zip\", \"sp_name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert some of the attributes to a more usable format\n",
    "used_cars_data[\"height\"] = used_cars_data[\"height\"].str.replace(r' in', '')\n",
    "used_cars_data[\"height\"] = pd.to_numeric(used_cars_data[\"height\"])\n",
    "\n",
    "# Then for the width...\n",
    "used_cars_data[\"width\"] = used_cars_data[\"width\"].str.replace(r' in', '')\n",
    "used_cars_data[\"width\"] = pd.to_numeric(used_cars_data[\"width\"])\n",
    "\n",
    "# ... and wheelbase\n",
    "used_cars_data[\"wheelbase\"] = used_cars_data[\"wheelbase\"].str.replace(r' in', '')\n",
    "used_cars_data[\"wheelbase\"] = pd.to_numeric(used_cars_data[\"wheelbase\"])\n",
    "\n",
    "# Next, the front_legroom attribute is converted\n",
    "used_cars_data[\"front_legroom\"] = used_cars_data[\"front_legroom\"].str.replace(r' in', '')\n",
    "used_cars_data[\"front_legroom\"] = pd.to_numeric(used_cars_data[\"front_legroom\"])\n",
    "\n",
    "# Convert the back_legroom attribute\n",
    "used_cars_data[\"back_legroom\"] = used_cars_data[\"back_legroom\"].str.replace(r' in', '')\n",
    "used_cars_data[\"back_legroom\"] = pd.to_numeric(used_cars_data[\"back_legroom\"])\n",
    "\n",
    "# Same process for the fuel_tank_volume attribue\n",
    "used_cars_data[\"fuel_tank_volume\"] = used_cars_data[\"fuel_tank_volume\"].str.replace(r' gal', '')\n",
    "used_cars_data[\"fuel_tank_volume\"] = pd.to_numeric(used_cars_data[\"fuel_tank_volume\"])\n",
    "\n",
    "# ... and power, removing \"hp\" and everything after it\n",
    "used_cars_data[\"power\"] = used_cars_data[\"power\"].astype(\"string\")\n",
    "used_cars_data[\"power\"] = used_cars_data[\"power\"].str[:-15]\n",
    "used_cars_data[\"power\"] = pd.to_numeric(used_cars_data[\"power\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything looks as intended after data cleaning\n",
    "used_cars_data.head(10)\n",
    "used_cars_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms can also be useful in understanding the data for numerical attributes\n",
    "used_cars_data.hist(bins=30, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data #1\n",
    "\n",
    "In exploring the data structure, some interesting and potentially relevant obersvations emerge. While it is important not to look to closely at the data before processing it in machine learning, accounting for how the human brain is very good at recognising patterns and could therefore easily be biased by findings if too much time is spent exploring the data structure, it is still important to extract the key point which will affect how we proceed. Interesting observations from exploring the data structure include:\n",
    "* Certain attributes have no null values, such as vin, price, city, make and year.\n",
    "* There are several attribute types, where many have the type string. This must be considere4d before completing future calculations on the data set.\n",
    "* Some attributes have a tail-heavy distribution, where most values are located on one side of the x-axis. Some examples of tail-heavy attributes are daysonmarket and year."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Test Set\n",
    "\n",
    "Creating a test set and a training set is essential to later on see how well the models perform on test data after having been trained on a portion of the data. The test set is therefore set aside early on, and not considered again until we are ready to test the models. It is common to use a 80-20 set, which we will do in this case too. However, there is still some more work to do before we can create the test and training set, so let us creste the test set, work some more with the data, and then set up a stratified test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the final version of the clean dataset to be equal to a new variable, called \"cars\"\n",
    "cars = used_cars_data\n",
    "cars = cars.fillna(cars.median())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important not to introduce sampling bias, when working with stratified sampling. This would be is data that is generally representative of the data no longer is representing the appropriate instances. In this scenario, we would want the attributes that might be important predictors of the price to stay representative of their respective categories. As such, to make sure certain categories maintain a matching distribution, we Pandas cut() method for the attribute milage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the process of creating a test set and a training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(cars, test_size=0.2, random_state=42)\n",
    "\n",
    "# Confirm the results look correct\n",
    "len(test_set) / len(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accounting for the representation of mileage, by creating categories for the mileage attribute\n",
    "cars[\"mileage_category\"] = pd.cut(cars[\"mileage\"], bins=[0.0, 50000.0, 100000.0, 150000.0, 200000.0, np.inf], labels=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the new bins we created contain enough values\n",
    "cars[\"mileage_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the newly created categories are created as intended\n",
    "cars[\"mileage_category\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mileage attribute has been added with the data type \"category,\" and must therefore be converted to a numerical data type\n",
    "print(cars.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datatype of the cars[\"mileage_category\"] from category to integer\n",
    "cars[\"mileage_category\"] = cars[\"mileage_category\"].astype('float64')\n",
    "print(cars.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no NaN values for our numeric values, in order to be able to do the StratifiedShuffleSplit\n",
    "# Check to see if there are any, and then we replace these with the median value, as not to affect the results\n",
    "print(cars.isna().sum())\n",
    "cars = cars.fillna(cars.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the algorithm for stratispied splitting from Scikit-learn to treat the newly created category\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "shuffled_data = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "[(train_index, test_index)] = shuffled_data.split(cars, cars[\"mileage_category\"])\n",
    "stratified_train_set = cars.loc[train_index]\n",
    "stratified_test_set = cars.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we see how we did for the test set...\n",
    "stratified_test_set[\"mileage_category\"].value_counts() / len(stratified_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and for the training set\n",
    "stratified_train_set[\"mileage_category\"].value_counts() / len(stratified_train_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering how the resulting values from testing the value counts in the two data sets are relatively similar, and we can therefore feel comfortable about the quality of the split. As such, we can move forward with process, and the next step would be to remove the newly created column, in addition to the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of the new column and the original dataset\n",
    "del(cars)\n",
    "stratified_train_set.drop(columns=\"mileage_category\", inplace=True)\n",
    "stratified_test_set.drop(\"mileage_category\", axis=1, inplace=True)\n",
    "\n",
    "stratified_train_set.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done so, it is time to store the test set, so that we do not look at it again accidentally or on purpose. If we need to or want to look back at the data, the only information available will be the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the training set\n",
    "cars = stratified_train_set.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data #2\n",
    "\n",
    "Having created a test set and put it aside, we can now take a proper look at the data, in order to get a better understanding of what attributes could possibly be correlated with the price. Exploring correlations will be a good place to start, to see what attributes look like they are most likely correlated with the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Standard Correlation Coefficient to start looking for correlation, to determine what attributes should be more closely investigated\n",
    "corr_matrix = cars.corr()\n",
    "\n",
    "# Explore the correlation for price, ordered by their value in descending order\n",
    "corr_matrix[\"price\"].sort_values(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting the results from the correlation, the results can range from 1 to -1, where results close to 1 indicates a strong positive correlation, and results close to -1 indicates a strong negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation using the scatter_matrix() function, but this is primarily doable on smaller datasets\n",
    "# Create one for the numeric attributes we believe to be most closely correlated with the price\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"price\", \"year\", \"horsepower\", \"power\", \"mileage\", \"longitude\"]\n",
    "pd.plotting.scatter_matrix(cars[attributes], figsize=(15, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mileage and year looks to be the most interesting attributes to explore further, based on the scatter matrix\n",
    "cars_correlations = cars.corr(numeric_only=True)\n",
    "cars_correlations\n",
    "\n",
    "# We are primarily interested in correlations to the price, and will therefore focus on that\n",
    "cars_correlations[\"price\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year looks to be closely related to price. We therefore take a closer look at this scatter plot\n",
    "plt.scatter(cars['price'], cars['year'])\n",
    "plt.title('Price vs. Year')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having explored the data, we see that year has a relatively strong correlation, while mileage has a relatively strong negative correlation. These are both helpful findings, as they are good indicators that these two attributes might have a significance in predicting the price of the used cars."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "Now, after having investigated the data, we need to prepare it. In doing so, we must separate the labels from the features, and drop the column that is for \"price\" itself. In preparing the data, we must also ensure that there are non NaN or non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate labels from the features\n",
    "cars_labels = cars[\"price\"].copy()\n",
    "cars_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column for the \"price\"\n",
    "cars = cars.drop(columns=\"price\")\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any of the data values are missing\n",
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some missing values, as not all attributes have all non-null values, and these are replaced with the median value\n",
    "fill_values = cars.select_dtypes(include=['int', 'float']).columns\n",
    "for col in fill_values:\n",
    "    median_val = cars[col].median()\n",
    "    cars[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# Check the data values again\n",
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving on to the boolean values and object values\n",
    "# As the imputer only wants numeric values, we must first drop these, then the array must be transformed into a data frame\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "drop_attributes = cars.select_dtypes(include=['bool', 'object']).columns\n",
    "cars_numerical = cars.drop(drop_attributes, axis=1)\n",
    "\n",
    "raw_output = imputer.fit_transform(cars_numerical)\n",
    "cars_imputed = pd.DataFrame(raw_output, columns=cars_numerical.columns, index=cars_numerical.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do a quality check, we check if any of the numerical attributes still have null values\n",
    "null_count = cars_numerical.isnull().sum()\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, we might still want to take a look at some of the boolean and object attributes\n",
    "# These must therefore be converted to a more useful format\n",
    "cars_categorical = cars[[\"franchise_dealer\", \"is_new\", \"body_type\", \"city\", \"engine_type\", \"fuel_type\", \"length\", \"listing_color\", \"make_name\", \"maximum_seating\", \"transmission\", \"wheel_system\", \"wheel_system_display\"]]\n",
    "\n",
    "cars_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In encoding these attributes, there are several ways this could be done. One way would be to encode the text into numbers with the OrdinalEncoder() method, but this might attribute too much weights to the numbers, which will skew the results. As such, we will use the one-hot encoding to replace the bollean and object attributes' true value with zeros and ones, indicating whether this attribute is present or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we cannot do calculations on values that are not numeric, we use the OneHodEncoder() method\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "categorical_encoder = OneHotEncoder()\n",
    "\n",
    "cars_categorical_encoded = categorical_encoder.fit_transform(cars_categorical)\n",
    "cars_categorical_encoded\n",
    "\n",
    "# Convert the result to an array, as it originally is returned as a sparse matrix\n",
    "cars_categorical_encoded.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we might want to scale our numerical features, as machine learning algorithms tend to work best when there is a similar scale to the numbers. We therefore modify the numerical features (Géron, 2019). The code is based on examples covered and learned in ID5059 lectures, where names of variables have been replaced with attributes more appropriate for the context of the US Used Cars dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # This example shows how to give the constructor an option\n",
    "    # The transformer will always add the rooms_per_household and population_per_household features\n",
    "    # We use an input argument to control whether to add (the default choice) or not the bedrooms_per_room feature\n",
    "    def __init__(self, add_mileage_per_year = True): \n",
    "        self.add_mileage_per_year = add_mileage_per_year\n",
    "\n",
    "    # This estimator only transforms, so the fit() method doesn't do anything\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    # Our transformation\n",
    "    def transform(self, X):\n",
    "\n",
    "        # The input dataset X is treated as a numpy array, so we will identify features by column number\n",
    "        years_index = 3\n",
    "        mileage_index = 4\n",
    "        make_name_index = 5\n",
    "        model_name_index = 6\n",
    "\n",
    "        # Compute two new features\n",
    "        years_per_model_name = X[:, years_index] / X[:, model_name_index]\n",
    "        make_name_per_model_name = X[:,  make_name_index] / X[:, model_name_index]\n",
    "\n",
    "        # Make a third new feature and then returns as output the input with all three new features added as new columns\n",
    "        if self.add_mileage_per_year:\n",
    "            mileage_per_year = X[:, mileage_index] / X[:, years_index]\n",
    "            return np.c_[X, years_per_model_name, make_name_per_model_name, mileage_per_year]\n",
    "\n",
    "        # Only returns as output the input with only the two new features added as new columns\n",
    "        else:\n",
    "            return np.c_[X, years_per_model_name, make_name_per_model_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Pipelines\n",
    "\n",
    "One example of a pipeline we could make for the dataset, is a preparation pipeline for data of numerical value, which fills inn null entries with the median value, adds new features better correlating to the labels and standardise everything. Such a pipeline is created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attributes_adder', CombinedAttributesAdder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pipeline by using the fit_transform method\n",
    "cars_numerical_transformed = numerical_pipeline.fit_transform(cars_numerical)\n",
    "\n",
    "# Check to see if everything looks right\n",
    "pd.DataFrame(cars_numerical_transformed).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the previous pipeline only can be used on numbers, we can create a new one for the categorical attributes\n",
    "# We can use scikit-learn's ColumnTransformer() method, built for combining such pipelines\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Since is_new was the only attribute that was not a float or an integer, but that still was of real interest in terms of correlation, we only bring this in the pipeline\n",
    "numerical_attributes = list(cars_numerical)\n",
    "categorical_attributes = [\"is_new\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"numerical\", numerical_pipeline, numerical_attributes),\n",
    "    (\"categorical\", OneHotEncoder(), categorical_attributes),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the whole pipeline looks like.\n",
    "cars_prepared = full_pipeline.fit_transform(cars)\n",
    "\n",
    "# Have a look at what the results look like...\n",
    "pd.DataFrame(cars_prepared).info()\n",
    "\n",
    "# ... and what the first few rows will display\n",
    "pd.DataFrame(cars_prepared).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now have more columns, which is likely related to the is_new attribute\n",
    "# Check to confirm this, and as suspected, it is a boolean value with only two possible options\n",
    "cars[\"is_new\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving on to Machine Learning Models\n",
    "Finally, after a lot of data exploration, cleaning and preparation, we can look more closely at some Machine Learning models, and how they interpret and perform on the dataset. We start off with Linear Regression, to generate some predictions for the cars test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a least squares linear regression model to the prepared training data of the cars\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(cars_prepared, cars_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some predictions\n",
    "data = cars.iloc[:5]\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the labels for the predictions\n",
    "labels = cars_labels.iloc[:5]\n",
    "pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use transform() instead of fit_transform to transform the test data, as not to transform the test data based on its properties instead of the full training set\n",
    "data_prepared = full_pipeline.transform(data)\n",
    "pd.DataFrame(data_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to make some predictions\n",
    "predictions = linear_regression.predict(data_prepared).round()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for comparison\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first test row predicts that the price is 22,768 when the actual value is 25,990. We move on to calculating the RMSE of predictions for the entirety of the training set, even though it is not really good practice to do testing on the trainingset. However, we will ignore this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_regression_cars_predictions = linear_regression.predict(cars_prepared)\n",
    "linear_regression_mse = mean_squared_error(cars_labels, linear_regression_cars_predictions)\n",
    "linear_regression_rmse = np.sqrt(linear_regression_mse)\n",
    "np.round(linear_regression_rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, this does not look very promising, as the typical error for the calculations is relatively high, compared to the price of the cars. Now, let us see if we are underfitting or overfittig the data, as this might be the cause for our problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(linear_regression_rmse / cars_labels.median(), 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, it becomes evident that we are underfitting the data. This could potentially be becuase the linear regression model is too simple for the dataset. We therefore move on, and try a different model. Let us see how the Decision Tree Model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and set up the model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_regressor = DecisionTreeRegressor(random_state=42)\n",
    "tree_regressor.fit(cars_prepared, cars_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the RMSE\n",
    "tree_regressor_cars_predictions = tree_regressor.predict(cars_prepared)\n",
    "tree_regressor_mse = mean_squared_error(cars_labels, tree_regressor_cars_predictions)\n",
    "tree_regressor_rmse = np.sqrt(tree_regressor_mse)\n",
    "tree_regressor_rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are overfitting the data. However, as the testing and creating of this code in Jupyter Notebook is done on a dataset that is way too small in a machine learning context, and that the \"smaller files extracted from the larger ones are not random samples.\" As such, we can be fairly certain this is inpacting the accuracy of our results when working with the small datasets. Now, let us still try with Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up and running the Cross Validation model for the used cars data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "K = 10\n",
    "\n",
    "tree_regressor_scores = cross_val_score(tree_regressor, cars_prepared, cars_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=K)\n",
    "tree_regressor_rmse_scores = np.sqrt(-tree_regressor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The K scores can be summarised through the creation of a function\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", np.round(scores))\n",
    "    print(\"Mean:\", np.round(scores.mean()))\n",
    "    print(\"Standard deviation:\", np.round(scores.std()))\n",
    "\n",
    "display_scores(tree_regressor_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare if the performance to the results of linear regression\n",
    "linear_regression_scores = cross_val_score(linear_regression, cars_prepared, cars_labels,\n",
    "                                           scoring=\"neg_mean_squared_error\", cv=K)\n",
    "linear_regression_rmse_scores = np.sqrt(-linear_regression_scores)\n",
    "display_scores(linear_regression_rmse_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation performs somewhat better than Linear Regression, even though none of them are yielding particularly qualitative results. Again, the factors of the size of the dataset, combined with how the data does not represent random samples, are fairly likely impacting the performance of our models. Let us still try with the Random Forest Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_regressor.fit(cars_prepared, cars_labels)\n",
    "\n",
    "forest_regressor_housing_predictions = forest_regressor.predict(cars_prepared)\n",
    "forest_regressor_mse = mean_squared_error(cars_labels, forest_regressor_housing_predictions)\n",
    "forest_regressor_rmse = np.sqrt(forest_regressor_mse)\n",
    "forest_regressor_rmse.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this model through the cross validation model as well\n",
    "forest_regressor_scores = cross_val_score(forest_regressor, cars_prepared, cars_labels,\n",
    "                                          scoring=\"neg_mean_squared_error\", cv=K)\n",
    "forest_regressor_rmse_scores = np.sqrt(-forest_regressor_scores)\n",
    "display_scores(forest_regressor_rmse_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Decision Tree Model looks to be the best model out of the ones we tried. The standard deviation for the Decision tree Model is less than what it was for Linear Regression and the Random Forest Model. However, we still note that the data still is likely to be overfit in terms of the entire training set, considering how the score was substantially better on that one. As the differences are so small, because of insufficient data, we still see how the Random Forest Model performs on the test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving and Fine-Tuning the Models\n",
    "\n",
    "It has been decided that we will move forward with the Random Forest Model, based on the results. We now want to experiment with the hyperparameters of the Random Forest Model. As such, we must decide on what hyperparameters we want to experiment with, as well as what values we want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and setting up the GridSearchCV function\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameter_grid = [\n",
    "    # Try 12 (3×4) combinations of hyperparameters:\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "\n",
    "    # Then try 6 (2×3) combinations with bootstrap set as False:\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(forest_regressor, parameter_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(cars_prepared, cars_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best hyperparameter values from the exploration so far\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the RMSE, to establish all trialled combinations possible\n",
    "grid_search_results = grid_search.cv_results_\n",
    "for mean_score, params in zip(grid_search_results[\"mean_test_score\"], grid_search_results[\"params\"]):\n",
    "    print(np.round(np.sqrt(-mean_score)), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the final model chosen to be the best out of the ones tried out\n",
    "final_model = grid_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Test Set\n",
    "\n",
    "Finally, it is time to try out the model on the test set. This has been set aside since we created it, as not to use or accidentally look at any of the data it contains, but now it is time to see how our chosen model performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model on the test data for the dataset\n",
    "X_test = stratified_test_set.drop(\"price\", axis=1)\n",
    "y_test = stratified_test_set[\"price\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare it to the mean\n",
    "np.round(final_rmse / y_test.mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the single RMSE value alone does not give any good indication for error variance, we try calculating the 95% confidence interval\n",
    "from scipy import stats\n",
    "\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "np.round(np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                                        loc=squared_errors.mean(),\n",
    "                                        scale=stats.sem(squared_errors))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Had this code been create for something other than an academic assessment setting, now would be the time to launch, monitor and maintain the system created. In this particular context, this is naturally not something we will do. However, we were able to explore the data structure, split it into a training set and a test set, clean the dataset, train a few models and find the best model from a few alternatives, when comparing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
